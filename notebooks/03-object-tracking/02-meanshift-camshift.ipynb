{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d6b2ba-a551-4215-89c1-38ae07b5d87e",
   "metadata": {},
   "source": [
    "<img src=\"../DATA/corp_logo.jpg\" width='160'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a5df32-cdf8-4337-8473-94f99d0918bf",
   "metadata": {},
   "source": [
    "# MeanShift\n",
    "There are a set of data points and we want to assign them into clusters. Imagine these data points are taken, and stack red and blue points on them(the red and blue points overlap completely in the first iteration before starting). The direction of the closest cluster centroid determined by where the most density of data points are at.\n",
    "\n",
    "So each iteration each blue point will move toward the most dense area(cluster ceneter).\n",
    "\n",
    "<img src=\"images/initial_state_meanshift.png\" >\n",
    "\n",
    "#### Iteration 1: \n",
    "At the end of iteration 1, all blue points are move towards the clusters.\n",
    "\n",
    "<img src=\"images/iteration_1_meanshift.png\" >\n",
    "\n",
    "...\n",
    "\n",
    "#### Iteration 4:\n",
    "After subsequent iterations, the cluster means converged.\n",
    "\n",
    "<img src=\"images/iteration_4_meanshift.png\">\n",
    "\n",
    "#### Iteration 5:\n",
    "Identified clusters.\n",
    "\n",
    "<img src=\"images/iteration_5_meanshift.png\">\n",
    "\n",
    "- Meanshift can be given a **target** to track, calculate the **histogram** of the target area, then the tracking window keep moving forward to the closest match(cluster center).\n",
    "- Meanshift **doesn't change the window size** if target moves in depth(**CAMshift** is used to **update** the size of the window).\n",
    "\n",
    "Check the link below for more information:\n",
    "https://medium.com/@claudio.vindimian/understanding-and-implementing-the-camshift-object-tracking-algorithm-python-81587c24eda8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7cd56-efe2-4dc8-8f3f-02f1ac7b01bd",
   "metadata": {},
   "source": [
    "# CAMShift Tracking\n",
    "\n",
    "It starts as the way before. After converging the Meanshift, an update in window size and fitting an eclipse to detect the orientation is performed. Agan, the whole process is repeated unitill some sort of accuracy is met .\n",
    "\n",
    "\n",
    "<img src='images/meanshift_converged.png'> &nbsp;&nbsp;&nbsp; <img src='images/roi_camshift.png'> &nbsp;&nbsp;&nbsp; <img src='images/orientation_camshift.png'> \n",
    "&nbsp;&nbsp;&nbsp;<img src='images/update_meanshift.png'> &nbsp;&nbsp;&nbsp; <img src='images/mean_shift_again.png'>\n",
    "\n",
    "---\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa111fe-96f8-4d21-afcb-3dd1b696c472",
   "metadata": {},
   "source": [
    "### MeanShift Tracking\n",
    "\n",
    ">**Note:**\n",
    ">\n",
    ">It is important to start a good position for detecting face for the first frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf950ace-76d4-4071-9834-262d4dbcb82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "\n",
    "# Capture a video stream\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# cap = cv2.VideoCapture()\n",
    "# cap.open(\"/dev/v4l/by-id/usb-Sonix_Technology_Co.__Ltd._USB_2.0_Camera-video-index0\")\n",
    "\n",
    "# take first frame of the video\n",
    "ret,frame = cap.read()\n",
    "\n",
    "\n",
    "# Set Up the Initial Tracking Window\n",
    "\n",
    "\n",
    "# We will first detect the face and set that as our starting box.\n",
    "face_cascade = cv2.CascadeClassifier('../DATA/haarcascades/haarcascade_frontalface_default.xml')\n",
    "face_rects = face_cascade.detectMultiScale(frame) \n",
    "\n",
    "# Convert this list of a single array to a tuple of (x,y,w,h)\n",
    "(face_x,face_y,w,h) = tuple(face_rects[0]) \n",
    "track_window = (face_x,face_y,w,h)\n",
    "# set up the ROI for tracking\n",
    "roi = frame[face_y:face_y+h, face_x:face_x+w]\n",
    "\n",
    "\n",
    "# Use the HSV Color Mapping\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Find histogram to backproject the target on each frame for calculation of meanshit\n",
    "roi_hist = cv2.calcHist([hsv_roi],[0],None,[180],[0,180])\n",
    "\n",
    "# Normalize the histogram array values given a min of 0 and max of 255\n",
    "cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)\n",
    "\n",
    "\n",
    "# Setup the termination criteria, either 10 iteration or move by at least 1 pt\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "\n",
    "while True:\n",
    "    ret ,frame = cap.read()\n",
    "    if ret == True:\n",
    "        \n",
    "        # Grab the Frame in HSV\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Calculate the Back Projection based off the roi_hist created earlier\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "        \n",
    "        # Apply meanshift to get the new coordinates of the rectangle\n",
    "        ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n",
    "        \n",
    "        # Draw the new rectangle on the image\n",
    "        x,y,w,h = track_window\n",
    "        img2 = cv2.rectangle(frame, (x,y), (x+w,y+h), (0,0,255),5)\n",
    "        \n",
    "        cv2.imshow('img2',img2)\n",
    "        \n",
    "        \n",
    "        k = cv2.waitKey(1) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548b3c3-fb03-4b95-a76d-43eb9b0f55c5",
   "metadata": {},
   "source": [
    "### CAMShift Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df705be0-18dd-4fff-b47a-83bd995464da",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m face_rects \u001b[38;5;241m=\u001b[39m face_cascade\u001b[38;5;241m.\u001b[39mdetectMultiScale(frame) \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convert this list of a single array to a tuple of (x,y,w,h)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m (face_x,face_y,w,h) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mface_rects\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m) \n\u001b[1;32m     23\u001b[0m track_window \u001b[38;5;241m=\u001b[39m (face_x,face_y,w,h)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# set up the ROI for tracking\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "\n",
    "# Capture a video stream\n",
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture()\n",
    "cap.open(\"/dev/v4l/by-id/usb-Sonix_Technology_Co.__Ltd._USB_2.0_Camera-video-index0\")\n",
    "\n",
    "\n",
    "# take first frame of the video\n",
    "ret,frame = cap.read()\n",
    "\n",
    "\n",
    "# Set Up the Initial Tracking Window\n",
    "\n",
    "\n",
    "# We will first detect the face and set that as our starting box.\n",
    "face_cascade = cv2.CascadeClassifier('../DATA/haarcascades/haarcascade_frontalface_default.xml')\n",
    "face_rects = face_cascade.detectMultiScale(frame) \n",
    "\n",
    "# Convert this list of a single array to a tuple of (x,y,w,h)\n",
    "(face_x,face_y,w,h) = tuple(face_rects[0]) \n",
    "track_window = (face_x,face_y,w,h)\n",
    "# set up the ROI for tracking\n",
    "roi = frame[face_y:face_y+h, face_x:face_x+w]\n",
    "\n",
    "\n",
    "# Use the HSV Color Mapping\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Find histogram to backproject the target on each frame for calculation of meanshit\n",
    "roi_hist = cv2.calcHist([hsv_roi],[0],None,[180],[0,180])\n",
    "\n",
    "# Normalize the histogram array values given a min of 0 and max of 255\n",
    "cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)\n",
    "\n",
    "\n",
    "# Setup the termination criteria, either 10 iteration or move by at least 1 pt\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "\n",
    "while True:\n",
    "    ret ,frame = cap.read()\n",
    "    if ret == True:\n",
    "        \n",
    "        # Grab the Frame in HSV\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Calculate the Back Projection based off the roi_hist created earlier\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "        \n",
    "        #########################################################\n",
    "        #########################################################\n",
    "        ########## CAM SHIFT ####################################\n",
    "        ########################################################\n",
    "        #######################################################\n",
    "        \n",
    "        # Apply Camshift to get the new coordinates of the rectangle\n",
    "        ret, track_window = cv2.CamShift(dst, track_window, term_crit)\n",
    "        \n",
    "        # Draw it on image\n",
    "        pts = cv2.boxPoints(ret)\n",
    "        pts = np.int0(pts)\n",
    "        img2 = cv2.polylines(frame,[pts],True, (0,0,255),5)\n",
    "        cv2.imshow('img2',img2)\n",
    "        \n",
    "        ########################################################\n",
    "        #######################################################\n",
    "        ########################################################\n",
    "        #######################################################\n",
    "        \n",
    "        k = cv2.waitKey(1) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
